{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef7e79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/tuan.tran1/Workspaces/Test/TestIceberg/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/tuan.tran1/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/tuan.tran1/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-gcp-bundle added as a dependency\n",
      "org.apache.iceberg#iceberg-gcp added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1bea8fe9-fd0b-4e55-9344-9a71135f8155;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.10.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-gcp-bundle;1.10.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-gcp;1.10.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-api;1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound org.apache.iceberg#iceberg-bundled-guava;1.10.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-common;1.10.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-core;1.10.1 in central\n",
      "\tfound org.apache.avro#avro;1.12.0 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.19.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.19.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.19.2 in central\n",
      "\tfound org.apache.commons#commons-compress;1.26.2 in central\n",
      "\tfound commons-codec#commons-codec;1.17.0 in central\n",
      "\tfound commons-io#commons-io;2.16.1 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-lang3;3.15.0 in central\n",
      "\tfound io.airlift#aircompressor;0.27 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents.client5#httpclient5;5.5 in central\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5;5.3.4 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents.core5#httpcore5-h2;5.3.4 in local-m2-cache\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.3 in local-m2-cache\n",
      "\tfound org.checkerframework#checker-qual;3.19.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.10.0 in local-m2-cache\n",
      "\tfound org.roaringbitmap#RoaringBitmap;1.3.0 in local-m2-cache\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.26.2/commons-compress-1.26.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.26.2!commons-compress.jar (305ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.17.0/commons-codec-1.17.0.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.17.0!commons-codec.jar (133ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.15.0/commons-lang3-3.15.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.15.0!commons-lang3.jar (150ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.19.0/checker-qual-3.19.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.19.0!checker-qual.jar (114ms)\n",
      ":: resolution report :: resolve 2965ms :: artifacts dl 733ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.19.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.19.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.19.2 from central in [default]\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.3 from local-m2-cache in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.10.0 from local-m2-cache in [default]\n",
      "\tcommons-codec#commons-codec;1.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.16.1 from local-m2-cache in [default]\n",
      "\tio.airlift#aircompressor;0.27 from local-m2-cache in [default]\n",
      "\torg.apache.avro#avro;1.12.0 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-compress;1.26.2 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.15.0 from central in [default]\n",
      "\torg.apache.httpcomponents.client5#httpclient5;5.5 from central in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5;5.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents.core5#httpcore5-h2;5.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.iceberg#iceberg-api;1.10.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-bundled-guava;1.10.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-common;1.10.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-core;1.10.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-gcp;1.10.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-gcp-bundle;1.10.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.10.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.19.0 from central in [default]\n",
      "\torg.roaringbitmap#RoaringBitmap;1.3.0 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;2.0.13 by [org.slf4j#slf4j-api;2.0.17] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.17] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   26  |   4   |   4   |   2   ||   24  |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1bea8fe9-fd0b-4e55-9344-9a71135f8155\n",
      "\tconfs: [default]\n",
      "\t22 artifacts copied, 2 already retrieved (14292kB/146ms)\n",
      "26/01/10 20:48:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CREATE SPARK SESSION WITH BIGQUERY METASTORE CATALOG\n",
    "# =============================================================================\n",
    "\n",
    "# Replace these with your actual values\n",
    "PROJECT_ID = \"rainbow-data-production-483609\"\n",
    "LOCATION = \"us-central1\"\n",
    "WAREHOUSE = \"gs://rainbow-data-production-iceberg/\"\n",
    "CATALOG_NAME = \"rainbow-data-production-iceberg\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"BigQuery Metastore Iceberg Example\")\n",
    "    .config(\"spark.master\", \"local[*]\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.port\", \"7078\")\n",
    "    .config(\"spark.blockManager.port\", \"7079\")\n",
    "    .config(\"spark.driver.memory\", \"2G\")\n",
    "    .config(\"spark.executor.memory\", \"2G\")\n",
    "    .config(\"spark.driver.userClassPathFirst\", \"false\")\n",
    "    .config(\"spark.executor.userClassPathFirst\", \"false\")\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    )\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{CATALOG_NAME}\", \"org.apache.iceberg.spark.SparkCatalog\"\n",
    "    )\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{CATALOG_NAME}.catalog-impl\",\n",
    "        \"org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    )\n",
    "    .config(f\"spark.sql.catalog.{CATALOG_NAME}.gcp.bigquery.project-id\", PROJECT_ID)\n",
    "    .config(f\"spark.sql.catalog.{CATALOG_NAME}.gcp.bigquery.location\", LOCATION)\n",
    "    .config(f\"spark.sql.catalog.{CATALOG_NAME}.warehouse\", WAREHOUSE)\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{CATALOG_NAME}.io-impl\",\n",
    "        \"org.apache.iceberg.gcp.gcs.GCSFileIO\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \",\".join(\n",
    "            [\n",
    "                \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1\",\n",
    "                \"org.apache.iceberg:iceberg-gcp-bundle:1.10.1\",\n",
    "                \"org.apache.iceberg:iceberg-gcp:1.10.1\"\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712a466a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot initialize Catalog implementation org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog: Cannot find constructor for interface org.apache.iceberg.catalog.Catalog\n\tMissing org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog [java.lang.ClassNotFoundException: org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSHOW NAMESPACES IN `rainbow-data-production-iceberg`\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspaces/Test/TestIceberg/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspaces/Test/TestIceberg/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspaces/Test/TestIceberg/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: Cannot initialize Catalog implementation org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog: Cannot find constructor for interface org.apache.iceberg.catalog.Catalog\n\tMissing org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog [java.lang.ClassNotFoundException: org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog]"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SHOW NAMESPACES IN `rainbow-data-production-iceberg`\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36028b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SHOW CATALOGS\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9371809f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[INVALID_IDENTIFIER] The identifier rainbow-data-production-iceberg is invalid. Please, consider quoting it with back-quotes as `rainbow-data-production-iceberg`.(line 1, pos 11)\n\n== SQL ==\nUSE rainbow-data-production-iceberg\n-----------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParseException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUSE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCATALOG_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspaces/Test/TestIceberg/.venv/lib/python3.12/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspaces/Test/TestIceberg/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspaces/Test/TestIceberg/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mParseException\u001b[39m: \n[INVALID_IDENTIFIER] The identifier rainbow-data-production-iceberg is invalid. Please, consider quoting it with back-quotes as `rainbow-data-production-iceberg`.(line 1, pos 11)\n\n== SQL ==\nUSE rainbow-data-production-iceberg\n-----------^^^\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"USE {CATALOG_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. CREATE A NAMESPACE (DATASET IN BIGQUERY)\n",
    "# =============================================================================\n",
    "\n",
    "# Switch to the catalog\n",
    "spark.sql(f\"USE {CATALOG_NAME}\")\n",
    "\n",
    "# Create a namespace (this creates a BigQuery dataset)\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS sales_data\")\n",
    "\n",
    "# List all namespaces\n",
    "print(\"=== Available Namespaces ===\")\n",
    "spark.sql(\"SHOW NAMESPACES\").show()\n",
    "\n",
    "# Use the namespace\n",
    "spark.sql(\"USE NAMESPACE sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd478c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. CREATE AN ICEBERG TABLE\n",
    "# =============================================================================\n",
    "\n",
    "# Option A: Create table using SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id BIGINT,\n",
    "        customer_id BIGINT,\n",
    "        product_name STRING,\n",
    "        quantity INT,\n",
    "        price DOUBLE,\n",
    "        order_date TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(order_date))\n",
    "    TBLPROPERTIES (\n",
    "        'write.format.default' = 'parquet',\n",
    "        'write.parquet.compression-codec' = 'snappy'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Option B: Create table from a DataFrame schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"customer_id\", LongType(), False),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customers (\n",
    "        customer_id BIGINT,\n",
    "        name STRING,\n",
    "        email STRING,\n",
    "        created_at TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "# List tables in the namespace\n",
    "print(\"=== Tables in sales_data namespace ===\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. INSERT DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Option A: Insert using SQL VALUES\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO orders VALUES\n",
    "    (1, 101, 'Laptop', 1, 1299.99, timestamp('2024-01-15 10:30:00')),\n",
    "    (2, 102, 'Mouse', 2, 29.99, timestamp('2024-01-15 11:45:00')),\n",
    "    (3, 101, 'Keyboard', 1, 79.99, timestamp('2024-01-16 09:00:00')),\n",
    "    (4, 103, 'Monitor', 1, 399.99, timestamp('2024-01-16 14:20:00')),\n",
    "    (5, 102, 'USB Cable', 3, 9.99, timestamp('2024-01-17 16:00:00'))\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO customers VALUES\n",
    "    (101, 'Alice Johnson', 'alice@example.com', timestamp('2024-01-01 08:00:00')),\n",
    "    (102, 'Bob Smith', 'bob@example.com', timestamp('2024-01-05 12:00:00')),\n",
    "    (103, 'Carol White', 'carol@example.com', timestamp('2024-01-10 15:00:00'))\n",
    "\"\"\")\n",
    "\n",
    "# Option B: Insert from a DataFrame\n",
    "from datetime import datetime\n",
    "\n",
    "order_data = [\n",
    "    (6, 104, \"Headphones\", 1, 149.99, datetime(2024, 1, 18, 10, 0, 0)),\n",
    "    (7, 105, \"Webcam\", 1, 89.99, datetime(2024, 1, 18, 11, 30, 0)),\n",
    "]\n",
    "\n",
    "order_schema = StructType(\n",
    "    [\n",
    "        StructField(\"order_id\", LongType(), False),\n",
    "        StructField(\"customer_id\", LongType(), False),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"quantity\", LongType(), True),\n",
    "        StructField(\"price\", DoubleType(), True),\n",
    "        StructField(\"order_date\", TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_new_orders = spark.createDataFrame(order_data, order_schema)\n",
    "df_new_orders.writeTo(f\"{CATALOG_NAME}.sales_data.orders\").append()\n",
    "\n",
    "print(\"=== Data inserted successfully ===\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. QUERY DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Simple SELECT\n",
    "print(\"=== All Orders ===\")\n",
    "spark.sql(\"SELECT * FROM orders ORDER BY order_id\").show()\n",
    "\n",
    "# Query with filters\n",
    "print(\"=== Orders over $100 ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, product_name, price\n",
    "    FROM orders\n",
    "    WHERE price > 100\n",
    "    ORDER BY price DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# JOIN tables\n",
    "print(\"=== Orders with Customer Names ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        c.name AS customer_name,\n",
    "        o.product_name,\n",
    "        o.quantity,\n",
    "        o.price,\n",
    "        o.order_date\n",
    "    FROM orders o\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    ORDER BY o.order_date\n",
    "\"\"\").show()\n",
    "\n",
    "# Aggregation\n",
    "print(\"=== Sales Summary by Customer ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.name AS customer_name,\n",
    "        COUNT(*) AS total_orders,\n",
    "        SUM(o.quantity) AS total_items,\n",
    "        ROUND(SUM(o.price * o.quantity), 2) AS total_spent\n",
    "    FROM orders o\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    GROUP BY c.name\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. UPDATE AND DELETE DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Update a record\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE orders \n",
    "    SET price = 1199.99 \n",
    "    WHERE order_id = 1\n",
    "\"\"\")\n",
    "print(\"=== After price update for order_id=1 ===\")\n",
    "spark.sql(\"SELECT * FROM orders WHERE order_id = 1\").show()\n",
    "\n",
    "# Delete a record\n",
    "spark.sql(\"DELETE FROM orders WHERE order_id = 5\")\n",
    "print(\"=== After deleting order_id=5 ===\")\n",
    "spark.sql(\"SELECT * FROM orders ORDER BY order_id\").show()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. MERGE (UPSERT) OPERATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create a temporary view with updates\n",
    "updates_data = [\n",
    "    (\n",
    "        1,\n",
    "        101,\n",
    "        \"Laptop Pro\",\n",
    "        1,\n",
    "        1499.99,\n",
    "        datetime(2024, 1, 15, 10, 30, 0),\n",
    "    ),  # Update existing\n",
    "    (8, 106, \"Tablet\", 1, 599.99, datetime(2024, 1, 19, 9, 0, 0)),  # New record\n",
    "]\n",
    "\n",
    "df_updates = spark.createDataFrame(updates_data, order_schema)\n",
    "df_updates.createOrReplaceTempView(\"order_updates\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO orders AS target\n",
    "    USING order_updates AS source\n",
    "    ON target.order_id = source.order_id\n",
    "    WHEN MATCHED THEN \n",
    "        UPDATE SET \n",
    "            product_name = source.product_name,\n",
    "            price = source.price\n",
    "    WHEN NOT MATCHED THEN \n",
    "        INSERT *\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== After MERGE operation ===\")\n",
    "spark.sql(\"SELECT * FROM orders ORDER BY order_id\").show()\n",
    "\n",
    "# =============================================================================\n",
    "# 8. SCHEMA EVOLUTION\n",
    "# =============================================================================\n",
    "\n",
    "# Add a new column\n",
    "spark.sql(\"ALTER TABLE orders ADD COLUMN discount DOUBLE\")\n",
    "\n",
    "# Update with new column\n",
    "spark.sql(\"UPDATE orders SET discount = 0.1 WHERE price > 500\")\n",
    "\n",
    "print(\"=== After schema evolution (added discount column) ===\")\n",
    "spark.sql(\n",
    "    \"SELECT order_id, product_name, price, discount FROM orders ORDER BY order_id\"\n",
    ").show()\n",
    "\n",
    "# =============================================================================\n",
    "# 9. TIME TRAVEL QUERIES\n",
    "# =============================================================================\n",
    "\n",
    "# View table history\n",
    "print(\"=== Table History (Snapshots) ===\")\n",
    "spark.sql(\"SELECT * FROM bq_catalog.sales_data.orders.history\").show(truncate=False)\n",
    "\n",
    "# View snapshots\n",
    "print(\"=== Snapshots ===\")\n",
    "spark.sql(\n",
    "    \"SELECT snapshot_id, committed_at, operation FROM bq_catalog.sales_data.orders.snapshots\"\n",
    ").show()\n",
    "\n",
    "# Query a specific snapshot (replace with actual snapshot_id from history)\n",
    "# spark.sql(\"SELECT * FROM orders VERSION AS OF <snapshot_id>\").show()\n",
    "\n",
    "# Query as of a timestamp\n",
    "# spark.sql(\"SELECT * FROM orders TIMESTAMP AS OF '2024-01-17 12:00:00'\").show()\n",
    "\n",
    "# =============================================================================\n",
    "# 10. TABLE MAINTENANCE\n",
    "# =============================================================================\n",
    "\n",
    "# Expire old snapshots (keep last 5)\n",
    "spark.sql(\"\"\"\n",
    "    CALL bq_catalog.system.expire_snapshots(\n",
    "        table => 'sales_data.orders',\n",
    "        older_than => TIMESTAMP '2024-12-31 00:00:00',\n",
    "        retain_last => 5\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Rewrite data files for optimization\n",
    "spark.sql(\"\"\"\n",
    "    CALL bq_catalog.system.rewrite_data_files(\n",
    "        table => 'sales_data.orders'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# 11. TABLE METADATA & INSPECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Describe table\n",
    "print(\"=== Table Schema ===\")\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED orders\").show(truncate=False)\n",
    "\n",
    "# View table properties\n",
    "print(\"=== Table Properties ===\")\n",
    "spark.sql(\"SHOW TBLPROPERTIES orders\").show(truncate=False)\n",
    "\n",
    "# View partitions\n",
    "print(\"=== Partitions ===\")\n",
    "spark.sql(\"SELECT * FROM bq_catalog.sales_data.orders.partitions\").show()\n",
    "\n",
    "# View files\n",
    "print(\"=== Data Files ===\")\n",
    "spark.sql(\n",
    "    \"SELECT file_path, file_format, record_count FROM bq_catalog.sales_data.orders.files\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# =============================================================================\n",
    "# 12. CLEANUP (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "# Drop tables\n",
    "# spark.sql(\"DROP TABLE IF EXISTS orders\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS customers\")\n",
    "\n",
    "# Drop namespace\n",
    "# spark.sql(\"DROP NAMESPACE IF EXISTS sales_data\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd94cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/tuan.tran1/Workspaces/Test/TestIceberg/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/tuan.tran1/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/tuan.tran1/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-gcp-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f1482565-cc86-4b60-b400-9fac21e9da57;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.10.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-gcp-bundle;1.10.1 in central\n",
      ":: resolution report :: resolve 115ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-gcp-bundle;1.10.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.10.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f1482565-cc86-4b60-b400-9fac21e9da57\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/14ms)\n",
      "26/01/08 13:45:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "catalog_name = \"rainbow-data-production-iceberg\"\n",
    "project_id = \"rainbow-data-production-483609\"\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark-Iceberg\")\n",
    "    .config(\"spark.master\", \"local[*]\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.port\", \"7078\")\n",
    "    .config(\"spark.blockManager.port\", \"7079\")\n",
    "    .config(\"spark.driver.memory\", \"2G\")\n",
    "    .config(\"spark.executor.memory\", \"2G\")\n",
    "    .config(\"spark.driver.userClassPathFirst\", \"false\")\n",
    "    .config(\"spark.executor.userClassPathFirst\", \"false\")\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\"\n",
    "    )\n",
    "    .config(f\"spark.sql.catalog.{catalog_name}.type\", \"rest\")\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{catalog_name}.uri\",\n",
    "        \"https://biglake.googleapis.com/iceberg/v1/restcatalog\",\n",
    "    )\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{catalog_name}.warehouse\",\n",
    "        f\"bq://projects/{project_id}\",\n",
    "    )\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{catalog_name}.header.x-goog-user-project\",\n",
    "        project_id,\n",
    "    )\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{catalog_name}.rest.auth.type\",\n",
    "        \"org.apache.iceberg.gcp.auth.GoogleAuthManager\",\n",
    "    )\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{catalog_name}.io-impl\",\n",
    "        \"org.apache.iceberg.gcp.gcs.GCSFileIO\",\n",
    "    )\n",
    "    .config(f\"spark.sql.catalog.{catalog_name}.rest-metrics-reporting-enabled\", \"false\")\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    )\n",
    "    .config(\"spark.sql.defaultCatalog\", catalog_name)\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \",\".join(\n",
    "            [\n",
    "                \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1\",\n",
    "                \"org.apache.iceberg:iceberg-gcp-bundle:1.10.1\",\n",
    "                # \"com.google.auth:google-auth-library-oauth2-http:1.41.0\",\n",
    "                # \"com.google.auth:google-auth-library-credentials:1.41.0\",\n",
    "                # \"com.google.guava:guava:32.1.2-jre\",\n",
    "                # \"com.google.cloud:google-cloud-storage:2.61.0\",\n",
    "                # \"com.google.cloud:libraries-bom:26.73.0\",\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a79ec350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             catalog|\n",
      "+--------------------+\n",
      "|rainbow-data-prod...|\n",
      "|       spark_catalog|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d2a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   current_catalog()|\n",
      "+--------------------+\n",
      "|rainbow-data-prod...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_catalog();\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d6460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"CREATE NAMESPACE IF NOT EXISTS test_namespace_cv ;\")\n",
    "spark.sql(\n",
    "    \"CREATE NAMESPACE IF NOT EXISTS test_namespace_cv LOCATION 'gs://rainbow-data-production-iceberg/test_namespace_cv' WITH DBPROPERTIES ('gcp-region' = 'us-central1');\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33981a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        namespace|\n",
      "+-----------------+\n",
      "|             test|\n",
      "|   test_namespace|\n",
      "|  test_namespace1|\n",
      "|test_namespace_cv|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW NAMESPACES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d05fd81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE test_namespace_cv;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7e56c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE test_namespace_cv;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72201c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"CREATE TABLE IF NOT EXISTS sample_table (id BIGINT, data STRING) USING ICEBERG;\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c0358dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+-----------+\n",
      "|        namespace|   tableName|isTemporary|\n",
      "+-----------------+------------+-----------+\n",
      "|test_namespace_cv|sample_table|      false|\n",
      "+-----------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eafc3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO sample_table VALUES\n",
    "  (1, 'first'), (2, 'second'), (3, 'third')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8badcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  data|\n",
      "+---+------+\n",
      "|  1| first|\n",
      "|  2|second|\n",
      "|  3| third|\n",
      "+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sample_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1b7559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO sample_table VALUES\n",
    "  (4, 'fourth'), (5, 'fifth'), (6, 'sixth');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "808bf8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  data|\n",
      "+---+------+\n",
      "|  1| first|\n",
      "|  2|second|\n",
      "|  3| third|\n",
      "|  4|fourth|\n",
      "|  5| fifth|\n",
      "|  6| sixth|\n",
      "+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sample_table;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503fc250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
